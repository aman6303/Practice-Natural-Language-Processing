{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695ad5bd-0b63-41d8-aec7-0de778699d0b",
   "metadata": {},
   "source": [
    "#### 1. Using split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7be65c-ec27-414e-b85a-411a086813e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi.', 'I', 'will']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = 'I am going to delhi. I will'\n",
    "sent1.split()  #splitting on space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfd73b-ec3d-4082-a6a6-1c70ce88c086",
   "metadata": {},
   "source": [
    "problem -- . is added to delhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3b3af5-edd8-4b62-811f-cff02e79159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')  #splitting on ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec292c-4091-4176-b60b-89c0da175ba8",
   "metadata": {},
   "source": [
    "fails in case of ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9e2cfe-ac2e-43a5-8556-18ad87f20112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64779d-1929-4a40-ac90-2fb43eaa16d2",
   "metadata": {},
   "source": [
    "#### 2.using regular exression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b98445-eaaf-4f5a-8dc8-d28869fd4a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word\n",
    "import re\n",
    "sent3 = 'I am going to delhi!'\n",
    "tokens = re.findall(r\"['\\w']+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69bd838f-cce2-4f94-99ed-285137935cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s.\"\"\"\n",
    "sentences = re.compile(r'[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8b010-e252-43e0-b979-9f1bac65b9a9",
   "metadata": {},
   "source": [
    "#### 3. using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faed854f-1559-498e-bdde-b4627a0e28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24ff0a4d-d688-4de7-9854-2d5639a3c7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'I have a Ph.D in A.I'\n",
    "sent2 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent3 = 'A 5km ride cost $10.50'\n",
    "\n",
    "print(word_tokenize(sent1))\n",
    "print(word_tokenize(sent2))\n",
    "print(word_tokenize(sent3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5e66d60-f7d4-49cd-a8e9-6a4bde062e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, annn amsj\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, annn amsj\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce610272-79df-4271-aebc-e95c7b44432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9f7f9-fef9-4213-8afd-ec9e3f997fa4",
   "metadata": {},
   "source": [
    "#### 4. using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b14c119c-dc9e-4eac-be96-e96f9df9a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm #first download eng lang model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09bd90b0-8ed1-408c-a4b6-7ebd13430883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29135802-07c1-4c23-81d1-691912ab03f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, have, a, Ph, ., D, in, A.I]\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'I have a Ph.D in A.I'\n",
    "tokens = [i for i in nlp(sent1)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817ab8b-73d4-4bff-9f1f-1d3ebc641a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60445cd1-eeee-40dd-bf6b-3805fdf88ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
